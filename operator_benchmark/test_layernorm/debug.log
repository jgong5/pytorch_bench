[compile_fx.py:75 INFO] Compiling FORWARDS graph
[scheduler.py:564 DEBUG] Creating schedule for node: ComputedBuffer(name='buf0', layout=FixedLayout('cpu', torch.float32, size=[s0, 1], stride=[1, s0]), data=Reduction(
  'cpu',
  torch.float32,
  load(arg0_1, r0 + i0 * s1),
  ranges=[s0, 1],
  reduction_ranges=[s1],
  reduction_type=sum,
  origins={var_mean}
))
[scheduler.py:564 DEBUG] Creating schedule for node: ComputedBuffer(name='buf1', layout=FixedLayout('cpu', torch.float32, size=[s0, 1], stride=[1, s0]), data=Reduction(
  'cpu',
  torch.float32,
  square(load(arg0_1, r0 + i0 * s1) - load(buf0, i0) / index_expr(s1, torch.float32)),
  ranges=[s0, 1],
  reduction_ranges=[s1],
  reduction_type=sum,
  origins={var_mean}
))
[scheduler.py:564 DEBUG] Creating schedule for node: ComputedBuffer(name='buf2', layout=FixedLayout('cpu', torch.float32, size=[s0, 1], stride=[1, s0]), data=Reduction(
  'cpu',
  torch.float32,
  load(arg0_1, r0 + i0 * s1),
  ranges=[s0, 1],
  reduction_ranges=[s1],
  reduction_type=sum,
  origins={var_mean}
))
[scheduler.py:564 DEBUG] Creating schedule for node: ComputedBuffer(name='buf3', layout=FixedLayout('cpu', torch.float32, size=[s0, s1], stride=[s1, 1]), data=Pointwise(
  'cpu',
  torch.float32,
  load(arg0_1, i1 + i0 * s1) - load(buf2, i0) / index_expr(s1, torch.float32) * reciprocal(sqrt(load(buf1, i0) / index_expr(s1, torch.float32) + constant(1e-05, torch.float32))) * load(arg1_1, i1) + load(arg2_1, i1),
  ranges=[s0, s1],
  origins={add_1}
))
[cpp.py:622 DEBUG] Creating new kernel for args: ([], [])
[cpp.py:451 DEBUG] deciding parallel depth for 28 threads:
[cpp.py:459 DEBUG]   expr: s0, size_hint: 384, parallel: 1, seq: 393216
[cpp.py:405 DEBUG] Parallel depth: 1:
[cpp.py:406 DEBUG] For(i0 in ks0, parallel 0, simd False, collapsed False, reduction_vars None)

[cpp.py:438 DEBUG] loads: auto tmp0 = in_ptr0[i1 + (i0*ks1)];

[cpp.py:439 DEBUG] compute: 
[cpp.py:440 DEBUG] stores: tmp1 += tmp0;

[cpp.py:622 DEBUG] Creating new kernel for args: (['in_ptr0', 'out_ptr0', 'ks0', 'ks1'], ['arg0_1', 'buf0', 's0', 's1'])
[cpp.py:451 DEBUG] deciding parallel depth for 28 threads:
[cpp.py:459 DEBUG]   expr: s0, size_hint: 384, parallel: 1, seq: 393216
[cpp.py:405 DEBUG] Parallel depth: 1:
[cpp.py:406 DEBUG] For(i0 in ks0, parallel 0, simd False, collapsed False, reduction_vars None)

[cpp.py:438 DEBUG] loads: auto tmp0 = in_ptr0[i1 + (i0*ks1)];
auto tmp1 = out_ptr0[i0];

[cpp.py:439 DEBUG] compute: auto tmp2 = static_cast<float>(ks1);
auto tmp3 = tmp1 / tmp2;
auto tmp4 = tmp0 - tmp3;
auto tmp5 = tmp4 * tmp4;

[cpp.py:440 DEBUG] stores: tmp6 += tmp5;
tmp7 += tmp0;

[cpp.py:622 DEBUG] Creating new kernel for args: (['in_ptr0', 'out_ptr0', 'out_ptr1', 'out_ptr2', 'ks0', 'ks1'], ['arg0_1', 'buf0', 'buf1', 'buf2', 's0', 's1'])
[cpp.py:451 DEBUG] deciding parallel depth for 28 threads:
[cpp.py:459 DEBUG]   expr: s0, size_hint: 384, parallel: 1, seq: 393216
[cpp.py:459 DEBUG]   expr: s1, size_hint: 1024, parallel: 384, seq: 1024.0
[cpp.py:405 DEBUG] Parallel depth: 1:
[cpp.py:406 DEBUG] For(i0 in ks0, parallel 0, simd False, collapsed False, reduction_vars None)
  For(i1 in ks1, parallel 0, simd True, collapsed False, reduction_vars None)

[cpp.py:438 DEBUG] loads: auto tmp0 = in_ptr0[i1 + (i0*ks1)];
auto tmp1 = out_ptr2[i0];
auto tmp5 = out_ptr1[i0];
auto tmp12 = in_ptr1[i1];
auto tmp14 = in_ptr2[i1];

[cpp.py:439 DEBUG] compute: auto tmp2 = static_cast<float>(ks1);
auto tmp3 = tmp1 / tmp2;
auto tmp4 = tmp0 - tmp3;
auto tmp6 = tmp5 / tmp2;
auto tmp7 = static_cast<float>(1e-05);
auto tmp8 = tmp6 + tmp7;
auto tmp9 = std::sqrt(tmp8);
auto tmp10 = 1 / tmp9;
auto tmp11 = tmp4 * tmp10;
auto tmp13 = tmp11 * tmp12;
auto tmp15 = tmp13 + tmp14;

[cpp.py:440 DEBUG] stores: out_ptr3[i1 + (i0*ks1)] = tmp15;

[graph.py:336 INFO] Output code: /tmp/torchinductor_jgong5/ei/ceiag3f2ucq5sn6wuj3rjcd2gqjhydrqytowjufxaqzj72ojdkwk.py
[debug.py:260 WARNING] model_inference_0 debug trace: /tmp/torchinductor_jgong5/ei/ceiag3f2ucq5sn6wuj3rjcd2gqjhydrqytowjufxaqzj72ojdkwk.debug
